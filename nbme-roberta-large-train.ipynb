{"cells":[{"cell_type":"markdown","metadata":{},"source":["version5 +Trends"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-03-26T13:55:38.939219Z","iopub.status.busy":"2022-03-26T13:55:38.938969Z","iopub.status.idle":"2022-03-26T13:55:45.550519Z","shell.execute_reply":"2022-03-26T13:55:45.549766Z","shell.execute_reply.started":"2022-03-26T13:55:38.939191Z"},"trusted":true},"outputs":[],"source":["import warnings\n","warnings.simplefilter('ignore')\n","\n","import os\n","import ast \n","from itertools import chain\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm.notebook import tqdm, trange\n","\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","import torch\n","from transformers import AutoModel, AutoTokenizer, AutoConfig"]},{"cell_type":"markdown","metadata":{},"source":["# CFG"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T14:00:05.716185Z","iopub.status.busy":"2022-03-26T14:00:05.71538Z","iopub.status.idle":"2022-03-26T14:00:05.72163Z","shell.execute_reply":"2022-03-26T14:00:05.720699Z","shell.execute_reply.started":"2022-03-26T14:00:05.716142Z"},"trusted":true},"outputs":[],"source":["#past_datasets=\"../input/roberta-large-fold0\"\n","seed = 41\n","\n","ROOT = '../input/nbme-score-clinical-patient-notes'\n","N_FOLDS = 5\n","#folds_idx=[0,1,2,3,4]\n","folds_idx=[3,4]\n","\n","\n","#MODEL_NAME = 'roberta-base'\n","MODEL_NAME = 'roberta-large'\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","BATCH_SIZE = 4\n","#BATCH_SIZE = 16\n","ACCUMULATION_STEPS = 2\n","EPOCHS = 5\n","#EPOCHS = 1\n","\n","#output_fn_cap=\"nbme_roberta_base\"\n","output_fn_cap=\"nbme_roberta_large\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T14:03:42.259825Z","iopub.status.busy":"2022-03-26T14:03:42.258988Z","iopub.status.idle":"2022-03-26T14:03:52.572237Z","shell.execute_reply":"2022-03-26T14:03:52.571152Z","shell.execute_reply.started":"2022-03-26T14:03:42.259779Z"},"trusted":true},"outputs":[],"source":["! cp ../input/nbme-roberta-large-seed41/nbme_roberta_large_0.pth nbme_roberta_large_0.pth\n","! cp ../input/nbme-roberta-large-seed41/nbme_roberta_large_1.pth nbme_roberta_large_1.pth\n","! cp ../input/nbme-roberta-large-seed41/nbme_roberta_large_2.pth nbme_roberta_large_2.pth"]},{"cell_type":"markdown","metadata":{},"source":["# data preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-09T03:36:30.092354Z","iopub.status.busy":"2022-02-09T03:36:30.091311Z","iopub.status.idle":"2022-02-09T03:36:31.671155Z","shell.execute_reply":"2022-02-09T03:36:31.670079Z","shell.execute_reply.started":"2022-02-09T03:36:30.092272Z"},"trusted":true},"outputs":[],"source":["def create_train_df(debug=False):\n","    feats = pd.read_csv(f\"{ROOT}/features.csv\")\n","    feats.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n","    \n","    notes = pd.read_csv(f\"{ROOT}/patient_notes.csv\")\n","    train = pd.read_csv(f\"{ROOT}/train.csv\")\n","    \n","    train['annotation'] = train['annotation'].apply(ast.literal_eval)\n","    train['location'] = train['location'].apply(ast.literal_eval)\n","    \n","    train.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\n","    train.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n","\n","    train.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\n","    train.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n","\n","    train.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n","    train.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n","\n","    train.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\n","    train.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n","\n","    train.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\n","    train.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n","\n","    train.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\n","    train.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n","\n","    train.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\n","    train.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n","\n","    train.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\n","    train.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n","\n","    train.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\n","    train.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n","\n","    train.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\n","    train.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n","\n","    train.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\n","    train.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n","\n","    train.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\n","    train.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n","\n","    train.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\n","    train.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n","\n","    train.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\n","    train.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n","\n","    train.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\n","    train.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n","\n","    train.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\n","    train.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n","\n","    train.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\n","    train.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n","\n","    train.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\n","    train.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n","\n","    train.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\n","    train.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n","\n","    train.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\n","    train.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n","\n","    train.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\n","    train.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n","\n","    train.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\n","    train.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n","\n","    train.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\n","    train.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n","\n","    train.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\n","    train.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n","\n","    train.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\n","    train.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n","\n","    train.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\n","    train.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n","\n","    train.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\n","    train.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n","\n","    train.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\n","    train.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n","\n","    train.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n","    train.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n","\n","    train.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\n","    train.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n","\n","    train.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\n","    train.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n","\n","    train.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n","    train.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n","\n","    train.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\n","    train.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n","\n","    train.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\n","    train.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n","\n","    train.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\n","    train.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n","\n","    train.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\n","    train.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n","\n","    train.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\n","    train.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n","\n","    train.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n","    train.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n","\n","    train.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\n","    train.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n","\n","    train.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\n","    train.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n","\n","    train.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\n","    train.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n","\n","    train.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\n","    train.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')\n","\n","    merged = train.merge(notes, how = \"left\")\n","    merged = merged.merge(feats, how = \"left\")\n","    \n","    def process_feature_text(text):\n","        return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n","    merged[\"feature_text\"] = [process_feature_text(x) for x in merged[\"feature_text\"]]\n","    \n","    merged[\"feature_text\"] = merged[\"feature_text\"].apply(lambda x: x.lower())\n","    merged[\"pn_history\"] = merged[\"pn_history\"].apply(lambda x: x.lower())\n","    \n","    if debug:\n","        merged = merged.sample(frac=0.5).reset_index(drop=True)\n","        \n","    skf = StratifiedKFold(n_splits=N_FOLDS,shuffle=True,random_state=seed)\n","    merged[\"stratify_on\"] = merged[\"case_num\"].astype(str) + merged[\"feature_num\"].astype(str)\n","    merged[\"fold\"] = -1\n","    for fold, (_, valid_idx) in enumerate(skf.split(merged[\"id\"], y=merged[\"stratify_on\"])):\n","        merged.loc[valid_idx, \"fold\"] = fold\n","    \n","    return merged\n","\n","train_df = create_train_df()\n","display(train_df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-09T03:36:31.673952Z","iopub.status.busy":"2022-02-09T03:36:31.672803Z","iopub.status.idle":"2022-02-09T03:36:31.681151Z","shell.execute_reply":"2022-02-09T03:36:31.680155Z","shell.execute_reply.started":"2022-02-09T03:36:31.673905Z"},"trusted":true},"outputs":[],"source":["def loc_list_to_ints(loc_list):\n","    to_return = []\n","    for loc_str in loc_list:\n","        loc_strs = loc_str.split(\";\")\n","        for loc in loc_strs:\n","            start, end = loc.split()\n","            to_return.append((int(start), int(end)))\n","    return to_return"]},{"cell_type":"markdown","metadata":{},"source":["# tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-09T03:36:39.892939Z","iopub.status.busy":"2022-02-09T03:36:39.892016Z","iopub.status.idle":"2022-02-09T03:36:39.909833Z","shell.execute_reply":"2022-02-09T03:36:39.908506Z","shell.execute_reply.started":"2022-02-09T03:36:39.892863Z"},"trusted":true},"outputs":[],"source":["def tokenize_and_add_labels(tokenizer, example):\n","    tokenized_inputs = tokenizer(\n","        example[\"feature_text\"],      # question\n","        example[\"pn_history\"],        # content\n","        truncation=\"only_second\",\n","        max_length=480,\n","        padding=\"max_length\",\n","        return_offsets_mapping=True\n","    )\n","    labels = [0.0] * len(tokenized_inputs[\"input_ids\"])\n","    tokenized_inputs[\"location_int\"] = loc_list_to_ints(example[\"location\"])\n","    tokenized_inputs[\"sequence_ids\"] = tokenized_inputs.sequence_ids()\n","    \n","    for idx, (seq_id, offsets) in enumerate(zip(tokenized_inputs[\"sequence_ids\"], tokenized_inputs[\"offset_mapping\"])):\n","        if seq_id is None or seq_id == 0:     # seq_id == None: special tokens | seq_id == 0: question\n","            labels[idx] = -100\n","            continue\n","        exit = False\n","        token_start, token_end = offsets\n","        for feature_start, feature_end in tokenized_inputs[\"location_int\"]:\n","            if exit:\n","                break\n","            if token_start >= feature_start and token_end <= feature_end:\n","                labels[idx] = 1.0\n","                exit = True\n","    tokenized_inputs[\"labels\"] = labels\n","    \n","    return tokenized_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-09T03:36:39.920005Z","iopub.status.busy":"2022-02-09T03:36:39.916417Z","iopub.status.idle":"2022-02-09T03:36:39.945461Z","shell.execute_reply":"2022-02-09T03:36:39.944025Z","shell.execute_reply.started":"2022-02-09T03:36:39.919939Z"},"trusted":true},"outputs":[],"source":["first = train_df.loc[0]\n","example = {\n","    \"feature_text\": first.feature_text,\n","    \"pn_history\": first.pn_history,\n","    \"location\": first.location,\n","    \"annotation\": first.annotation\n","}\n","for key in example.keys():\n","    print(key)\n","    print(example[key])\n","    print(\"=\" * 100)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-09T03:36:39.952509Z","iopub.status.busy":"2022-02-09T03:36:39.951681Z","iopub.status.idle":"2022-02-09T03:36:39.987438Z","shell.execute_reply":"2022-02-09T03:36:39.986164Z","shell.execute_reply.started":"2022-02-09T03:36:39.952238Z"},"trusted":true},"outputs":[],"source":["tokenized_inputs = tokenize_and_add_labels(tokenizer, example)\n","for key in tokenized_inputs.keys():\n","    print(key)\n","    print(tokenized_inputs[key])\n","    print(\"=\" * 100)"]},{"cell_type":"markdown","metadata":{},"source":["# dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-09T03:36:39.994425Z","iopub.status.busy":"2022-02-09T03:36:39.993766Z","iopub.status.idle":"2022-02-09T03:36:40.01635Z","shell.execute_reply":"2022-02-09T03:36:40.014824Z","shell.execute_reply.started":"2022-02-09T03:36:39.994378Z"},"trusted":true},"outputs":[],"source":["class NBMEData(torch.utils.data.Dataset):\n","    def __init__(self, data, tokenizer):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        \n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def __getitem__(self, idx):\n","        example = self.data.loc[idx]\n","        tokenized = tokenize_and_add_labels(self.tokenizer, example)\n","        \n","        input_ids = np.array(tokenized[\"input_ids\"])\n","        attention_mask = np.array(tokenized[\"attention_mask\"])\n","        labels = np.array(tokenized[\"labels\"])\n","        offset_mapping = np.array(tokenized[\"offset_mapping\"])\n","        sequence_ids = np.array(tokenized[\"sequence_ids\"]).astype(\"float16\")\n","        \n","        return input_ids, attention_mask, labels, offset_mapping, sequence_ids"]},{"cell_type":"markdown","metadata":{},"source":["# model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-09T03:36:40.02899Z","iopub.status.busy":"2022-02-09T03:36:40.023602Z","iopub.status.idle":"2022-02-09T03:36:40.045981Z","shell.execute_reply":"2022-02-09T03:36:40.043569Z","shell.execute_reply.started":"2022-02-09T03:36:40.028945Z"},"trusted":true},"outputs":[],"source":["class NBMEModel(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.backbone = AutoModel.from_pretrained(MODEL_NAME)\n","        self.config = AutoConfig.from_pretrained(MODEL_NAME)\n","        self.dropout = torch.nn.Dropout(p=0.2)\n","        self.classifier = torch.nn.Linear(self.config.hidden_size, 1)\n","        \n","    def forward(self, input_ids, attention_mask):\n","        pooler_outputs = self.backbone(input_ids=input_ids, \n","                                       attention_mask=attention_mask)[0]\n","        logits = self.classifier(self.dropout(pooler_outputs)).squeeze(-1)\n","        return logits"]},{"cell_type":"markdown","metadata":{},"source":["# training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-09T03:36:40.055098Z","iopub.status.busy":"2022-02-09T03:36:40.054826Z","iopub.status.idle":"2022-02-09T03:36:40.124571Z","shell.execute_reply":"2022-02-09T03:36:40.123206Z","shell.execute_reply.started":"2022-02-09T03:36:40.055067Z"},"trusted":true},"outputs":[],"source":["DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","DEVICE"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-09T03:36:40.140475Z","iopub.status.busy":"2022-02-09T03:36:40.13916Z","iopub.status.idle":"2022-02-09T03:36:40.167638Z","shell.execute_reply":"2022-02-09T03:36:40.166394Z","shell.execute_reply.started":"2022-02-09T03:36:40.140426Z"},"trusted":true},"outputs":[],"source":["class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n = 1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))\n","\n","\n","def get_location_predictions(preds, offset_mapping, sequence_ids, test = False):\n","    all_predictions = []\n","    for pred, offsets, seq_ids in zip(preds, offset_mapping, sequence_ids):\n","        pred = sigmoid(pred)\n","        start_idx = None\n","        current_preds = []\n","        for p, o, s_id in zip(pred, offsets, seq_ids):\n","            if s_id is None or s_id == 0:\n","                continue\n","            if p > 0.5:\n","                if start_idx is None:\n","                    start_idx = o[0]\n","                end_idx = o[1]\n","            elif start_idx is not None:\n","                if test:\n","                    current_preds.append(f\"{start_idx} {end_idx}\")\n","                else:\n","                    current_preds.append((start_idx, end_idx))\n","                start_idx = None\n","        if test:\n","            all_predictions.append(\"; \".join(current_preds))\n","        else:\n","            all_predictions.append(current_preds)\n","    return all_predictions\n","\n","\n","def calculate_char_CV(predictions, offset_mapping, sequence_ids, labels):\n","    all_labels = []\n","    all_preds = []\n","    for preds, offsets, seq_ids, labels in zip(predictions, offset_mapping, sequence_ids, labels):\n","        num_chars = max(list(chain(*offsets)))\n","        char_labels = np.zeros((num_chars))\n","        for o, s_id, label in zip(offsets, seq_ids, labels):\n","            if s_id is None or s_id == 0:\n","                continue\n","            if int(label) == 1:\n","                char_labels[o[0]:o[1]] = 1\n","        char_preds = np.zeros((num_chars))\n","        for start_idx, end_idx in preds:\n","            char_preds[start_idx:end_idx] = 1\n","        all_labels.extend(char_labels)\n","        all_preds.extend(char_preds)\n","    results = precision_recall_fscore_support(all_labels, all_preds, average = \"binary\")\n","    return {\n","        \"precision\": results[0],\n","        \"recall\": results[1],\n","        \"f1\": results[2]\n","    }\n","\n","\n","def compute_metrics(p):\n","    predictions, y_true = p\n","    y_true = y_true.astype(int)\n","    y_pred = [\n","        [int(p > 0.5) for (p, l) in zip(pred, label) if l != -100]\n","        for pred, label in zip(predictions, y_true)\n","    ]\n","    y_true = [\n","        [l for l in label if l != -100] for label in y_true\n","    ]\n","    results = precision_recall_fscore_support(list(chain(*y_true)), list(chain(*y_pred)), average = \"binary\")\n","    return {\n","        \"token_precision\": results[0],\n","        \"token_recall\": results[1],\n","        \"token_f1\": results[2]\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-09T03:36:40.173562Z","iopub.status.busy":"2022-02-09T03:36:40.172807Z","iopub.status.idle":"2022-02-09T03:36:40.20102Z","shell.execute_reply":"2022-02-09T03:36:40.199942Z","shell.execute_reply.started":"2022-02-09T03:36:40.173497Z"},"trusted":true},"outputs":[],"source":["def train_fn(fold):\n","    \n","    # get dataloader\n","    train = train_df[train_df[\"fold\"] != fold].reset_index(drop=True)\n","    valid = train_df[train_df[\"fold\"] == fold].reset_index(drop=True)\n","    train_ds = NBMEData(train, tokenizer)\n","    valid_ds = NBMEData(valid, tokenizer)\n","    train_dl = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, \n","                                           pin_memory=True, shuffle=True, drop_last=True)\n","    valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=BATCH_SIZE * 2, \n","                                           pin_memory=True, shuffle=False, drop_last=False)\n","    \n","    # get model\n","    model = NBMEModel().to(DEVICE)\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n","    loss_fn = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n","    \n","    history = {\"train\": [], \"valid\": []}\n","    best_loss = np.inf\n","    \n","    # training\n","    for epoch in range(EPOCHS):\n","        model.train()\n","        train_loss = AverageMeter()\n","        pbar = tqdm(train_dl)\n","        for i, batch in enumerate(pbar):\n","            input_ids = batch[0].to(DEVICE)\n","            attention_mask = batch[1].to(DEVICE)\n","            labels = batch[2].to(DEVICE)\n","            logits = model(input_ids, attention_mask)\n","            loss = loss_fn(logits, labels)\n","            loss = torch.masked_select(loss, labels > -1).mean()\n","            loss /= ACCUMULATION_STEPS\n","            loss.backward()\n","            if (i+1) % ACCUMULATION_STEPS == 0: \n","                optimizer.step()\n","                optimizer.zero_grad()\n","            train_loss.update(val=loss.item(), n=len(input_ids))\n","            pbar.set_postfix(Loss=train_loss.avg)\n","        print(f\"EPOCH: {epoch} train loss: {train_loss.avg}\")\n","        history[\"train\"].append(train_loss.avg)\n","        \n","        # evaluation\n","        model.eval()\n","        valid_loss = AverageMeter()\n","        pbar = tqdm(valid_dl)\n","        with torch.no_grad():\n","            for i, batch in enumerate(pbar):\n","                input_ids = batch[0].to(DEVICE)\n","                attention_mask = batch[1].to(DEVICE)\n","                labels = batch[2].to(DEVICE)\n","                logits = model(input_ids, attention_mask)\n","                loss = loss_fn(logits, labels)\n","                loss = torch.masked_select(loss, labels > -1).mean()\n","                valid_loss.update(val=loss.item(), n=len(input_ids))\n","                pbar.set_postfix(Loss=valid_loss.avg)\n","        print(f\"EPOCH: {epoch} valid loss: {valid_loss.avg}\")\n","        history[\"valid\"].append(valid_loss.avg)\n","\n","        # save model\n","        if valid_loss.avg < best_loss:\n","            best_loss = valid_loss.avg\n","            torch.save(model.state_dict(), output_fn_cap+f\"_{fold}.pth\")\n","        \n","    # evaluation summary\n","    model.load_state_dict(torch.load(output_fn_cap+f\"_{fold}.pth\", map_location = DEVICE))\n","    model.eval()\n","    preds = []\n","    offsets = []\n","    seq_ids = []\n","    lbls = []\n","    with torch.no_grad():\n","        for batch in tqdm(valid_dl):\n","            input_ids = batch[0].to(DEVICE)\n","            attention_mask = batch[1].to(DEVICE)\n","            labels = batch[2].to(DEVICE)\n","            offset_mapping = batch[3]\n","            sequence_ids = batch[4]\n","            logits = model(input_ids, attention_mask)\n","            preds.append(logits.cpu().numpy())\n","            offsets.append(offset_mapping.numpy())\n","            seq_ids.append(sequence_ids.numpy())\n","            lbls.append(labels.cpu().numpy())\n","    preds = np.concatenate(preds, axis=0)\n","    offsets = np.concatenate(offsets, axis=0)\n","    seq_ids = np.concatenate(seq_ids, axis=0)\n","    lbls = np.concatenate(lbls, axis=0)\n","    location_preds = get_location_predictions(preds, offsets, seq_ids, test=False)\n","    score = calculate_char_CV(location_preds, offsets, seq_ids, lbls)\n","    print(f\"Fold: {fold} CV score: {score}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-09T03:36:40.203172Z","iopub.status.busy":"2022-02-09T03:36:40.202427Z"},"trusted":true},"outputs":[],"source":["# for fold in range(N_FOLDS):\n","for fold in folds_idx:\n","    train_fn(fold)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
